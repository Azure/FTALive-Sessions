{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae52cb2f",
   "metadata": {},
   "source": [
    "# Azure OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd67747",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "There are two main use cases this playbook will cover: document search and document “zone”. \n",
    "This playbook will walk through an example of querying against a knowledge base to find the most relevant document (document search) and show once a document is isolated how to find the most relevant section within the document (document zone).\n",
    "\n",
    "### Document Search ###\n",
    "Enterprise document search is the process of bringing a seamless search experience to finding and retrieving relevant documentation, data, and knowledge that are stored in various formats across databases within an organization. Document search can empower your team to quickly find resources across the organization through a query in natural language, presented in a holistic view.\n",
    "Document search is imperative for Enterprises that deal with significant amounts of documentation such as law firms, large businesses, and public sector entities. Overall, enterprise search is important for businesses because it saves time, harnesses valuable knowledge, and provides a seamless user experience.\n",
    "\n",
    "### Document “Zone”\n",
    "For many enterprises documents can span across tens of information dense pages. Once a document search has been performed and a document has been isolated based on a query, it is essential to zone in on the right page or section of the document to gather the relevant information or pass it through to a summarization tool. \n",
    "Isolating specific pages or sections within a long document to answer a user query can ensure a succinct response, less computational time of summarizing or extracting the entire document and helping save valuable time and resources for the individual querying the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd434cc1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461e453a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../src/.env') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cb38c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Warning Shown\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "print(\"No Warning Shown\")\n",
    "\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_KEY\")  # SET YOUR OWN API KEY HERE\n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  # SET A LINK TO YOUR RESOURCE ENDPOINT\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_version = \"2022-06-01-preview\"\n",
    "\n",
    "TEXT_SEARCH_EMBEDDING_ENGINE = 'text-search-curie-doc-001'\n",
    "COMPLETIONS_MODEL = \"text-davinci-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc18ad",
   "metadata": {},
   "source": [
    "## Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cab1b4",
   "metadata": {},
   "source": [
    "### Use Case Overview\n",
    "This section will go over how to use Azure OpenAI embeddings for the document search use case. The goal of this section is given a knowledge base with documents and an user query, to isolate to the search result which can answer the question presented in the query.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "The first dataset we will look at is the BillSum dataset. BillSum is the first dataset for summarization of US Congressional and California state bills. For illustration purposes, we will look at the US bills solely. The corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills.  The BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 characters in length.\n",
    "More information on the dataset and downloading instructions can be found here: 3\n",
    "Schema\n",
    "-\tbill_id: an identifier for the bill\n",
    "-\ttext: US bill text\n",
    "-\tsummary: human written bill summary\n",
    "-\ttitle: bill title\n",
    "-\ttext_len: character length of the bill\n",
    "-\tsum_len: character length of the bill summary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be94b371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       20\n",
       "summary    20\n",
       "title      20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets can be found under the data/ directory\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), 'data', 'bill_sum_data.csv'))\n",
    "df_bills = df[['text', 'summary', 'title']]\n",
    "df_bills.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b043fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform light data cleaning (removing redudant whitespace and cleaning up punctuation)\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df_bills['text'] = df_bills[\"text\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bc26db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1480 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# remove bills that are too long for the token limitation\n",
    "df_bills['n_tokens'] = df_bills[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_bills = df_bills[df_bills.n_tokens<2000]\n",
    "len(df_bills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923279fd",
   "metadata": {},
   "source": [
    "Before the search, we will embed the text documents and save the corresponding embedding. We embed each chunk using a ‘doc’ model (i.e. text-search-curie-doc-001). \n",
    "These embeddings can be stored locally or in an Azure DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f00d21ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>1152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...</td>\n",
       "      <td>Requires the Director of National Intelligence...</td>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Military Call-up Relief Act - Amends the Inter...</td>\n",
       "      <td>A bill to amend the Internal Revenue Code of 1...</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "1  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "2  SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...   \n",
       "4  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  National Science Education Tax Incentive for B...   \n",
       "1  Small Business Expansion and Hiring Act of 201...   \n",
       "2  Requires the Director of National Intelligence...   \n",
       "4  Military Call-up Relief Act - Amends the Inter...   \n",
       "\n",
       "                                               title  n_tokens  \n",
       "0  To amend the Internal Revenue Code of 1986 to ...      1480  \n",
       "1  To amend the Internal Revenue Code of 1986 to ...      1152  \n",
       "2  A bill to require the Director of National Int...       930  \n",
       "4  A bill to amend the Internal Revenue Code of 1...      1048  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bills.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da0ef97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bills['curie_search'] = df_bills[\"text\"].apply(lambda x : get_embedding(x, engine = TEXT_SEARCH_EMBEDDING_ENGINE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b22f05c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>curie_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>1480</td>\n",
       "      <td>[-0.019770914688706398, 0.011169900186359882, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>1152</td>\n",
       "      <td>[-0.007850012741982937, 0.01001765951514244, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...</td>\n",
       "      <td>Requires the Director of National Intelligence...</td>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>930</td>\n",
       "      <td>[0.00012103027984267101, 0.011845593340694904,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Military Call-up Relief Act - Amends the Inter...</td>\n",
       "      <td>A bill to amend the Internal Revenue Code of 1...</td>\n",
       "      <td>1048</td>\n",
       "      <td>[-0.005481021944433451, 0.00856819562613964, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SECTION 1. RELIQUIDATION OF CERTAIN ENTRIES PR...</td>\n",
       "      <td>Requires the Customs Service to reliquidate ce...</td>\n",
       "      <td>To provide for reliquidation of entries premat...</td>\n",
       "      <td>1846</td>\n",
       "      <td>[-0.008310390636324883, -0.004660653416067362,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Service Dogs for Veterans Act of 2009 - Direct...</td>\n",
       "      <td>A bill to require the Secretary of Veterans Af...</td>\n",
       "      <td>872</td>\n",
       "      <td>[-0.017687108367681503, 0.011164870113134384, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Taxpayer's Right to View Act of 1993 - Amends ...</td>\n",
       "      <td>Taxpayer's Right to View Act of 1993</td>\n",
       "      <td>946</td>\n",
       "      <td>[0.0021867561154067516, -0.004219848196953535,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SECTION 1. FINDINGS. The Congress finds the fo...</td>\n",
       "      <td>Amends the Marine Mammal Protection Act of 197...</td>\n",
       "      <td>To amend the Marine Mammal Protection Act of 1...</td>\n",
       "      <td>1223</td>\n",
       "      <td>[-0.015813011676073074, 0.009919906966388226, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Education and Training for Health Act of 2017 ...</td>\n",
       "      <td>Education and Training for Health Act of 2017</td>\n",
       "      <td>1596</td>\n",
       "      <td>[-0.0150684155523777, 0.005073960404843092, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Andrew Prior Act or Andrew's Law - Amends the ...</td>\n",
       "      <td>Andrew's Law</td>\n",
       "      <td>608</td>\n",
       "      <td>[-0.011593054980039597, 0.022752899676561356, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Directs the President, in coordination with de...</td>\n",
       "      <td>Energy Independence Act of 2000</td>\n",
       "      <td>1341</td>\n",
       "      <td>[-0.008348068222403526, 0.00272438395768404, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>This measure has not been amended since it was...</td>\n",
       "      <td>Veterans Entrepreneurship Act of 2015</td>\n",
       "      <td>1404</td>\n",
       "      <td>[-0.020315825939178467, 0.0011716989101842046,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "1   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "2   SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...   \n",
       "4   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "5   SECTION 1. RELIQUIDATION OF CERTAIN ENTRIES PR...   \n",
       "6   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "9   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "12  SECTION 1. FINDINGS. The Congress finds the fo...   \n",
       "14  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "16  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "17  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "18  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   National Science Education Tax Incentive for B...   \n",
       "1   Small Business Expansion and Hiring Act of 201...   \n",
       "2   Requires the Director of National Intelligence...   \n",
       "4   Military Call-up Relief Act - Amends the Inter...   \n",
       "5   Requires the Customs Service to reliquidate ce...   \n",
       "6   Service Dogs for Veterans Act of 2009 - Direct...   \n",
       "9   Taxpayer's Right to View Act of 1993 - Amends ...   \n",
       "12  Amends the Marine Mammal Protection Act of 197...   \n",
       "14  Education and Training for Health Act of 2017 ...   \n",
       "16  Andrew Prior Act or Andrew's Law - Amends the ...   \n",
       "17  Directs the President, in coordination with de...   \n",
       "18  This measure has not been amended since it was...   \n",
       "\n",
       "                                                title  n_tokens  \\\n",
       "0   To amend the Internal Revenue Code of 1986 to ...      1480   \n",
       "1   To amend the Internal Revenue Code of 1986 to ...      1152   \n",
       "2   A bill to require the Director of National Int...       930   \n",
       "4   A bill to amend the Internal Revenue Code of 1...      1048   \n",
       "5   To provide for reliquidation of entries premat...      1846   \n",
       "6   A bill to require the Secretary of Veterans Af...       872   \n",
       "9                Taxpayer's Right to View Act of 1993       946   \n",
       "12  To amend the Marine Mammal Protection Act of 1...      1223   \n",
       "14      Education and Training for Health Act of 2017      1596   \n",
       "16                                       Andrew's Law       608   \n",
       "17                    Energy Independence Act of 2000      1341   \n",
       "18              Veterans Entrepreneurship Act of 2015      1404   \n",
       "\n",
       "                                         curie_search  \n",
       "0   [-0.019770914688706398, 0.011169900186359882, ...  \n",
       "1   [-0.007850012741982937, 0.01001765951514244, 0...  \n",
       "2   [0.00012103027984267101, 0.011845593340694904,...  \n",
       "4   [-0.005481021944433451, 0.00856819562613964, -...  \n",
       "5   [-0.008310390636324883, -0.004660653416067362,...  \n",
       "6   [-0.017687108367681503, 0.011164870113134384, ...  \n",
       "9   [0.0021867561154067516, -0.004219848196953535,...  \n",
       "12  [-0.015813011676073074, 0.009919906966388226, ...  \n",
       "14  [-0.0150684155523777, 0.005073960404843092, 0....  \n",
       "16  [-0.011593054980039597, 0.022752899676561356, ...  \n",
       "17  [-0.008348068222403526, 0.00272438395768404, 0...  \n",
       "18  [-0.020315825939178467, 0.0011716989101842046,...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f6902",
   "metadata": {},
   "source": [
    "At the time of search (live compute), we will embed the search query using the corresponding ‘query’ model (text-serach-query-001). Next find the closest embedding in the database, ranked by cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0923f5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>curie_search</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Taxpayer's Right to View Act of 1993 - Amends ...</td>\n",
       "      <td>Taxpayer's Right to View Act of 1993</td>\n",
       "      <td>946</td>\n",
       "      <td>[0.0021867561154067516, -0.004219848196953535,...</td>\n",
       "      <td>0.363270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>1480</td>\n",
       "      <td>[-0.019770914688706398, 0.011169900186359882, ...</td>\n",
       "      <td>0.314105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>1152</td>\n",
       "      <td>[-0.007850012741982937, 0.01001765951514244, 0...</td>\n",
       "      <td>0.297908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SECTION 1. SHORT TITLE. This Act may be cited ...</td>\n",
       "      <td>This measure has not been amended since it was...</td>\n",
       "      <td>Veterans Entrepreneurship Act of 2015</td>\n",
       "      <td>1404</td>\n",
       "      <td>[-0.020315825939178467, 0.0011716989101842046,...</td>\n",
       "      <td>0.295586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "9   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "0   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "1   SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "18  SECTION 1. SHORT TITLE. This Act may be cited ...   \n",
       "\n",
       "                                              summary  \\\n",
       "9   Taxpayer's Right to View Act of 1993 - Amends ...   \n",
       "0   National Science Education Tax Incentive for B...   \n",
       "1   Small Business Expansion and Hiring Act of 201...   \n",
       "18  This measure has not been amended since it was...   \n",
       "\n",
       "                                                title  n_tokens  \\\n",
       "9                Taxpayer's Right to View Act of 1993       946   \n",
       "0   To amend the Internal Revenue Code of 1986 to ...      1480   \n",
       "1   To amend the Internal Revenue Code of 1986 to ...      1152   \n",
       "18              Veterans Entrepreneurship Act of 2015      1404   \n",
       "\n",
       "                                         curie_search  similarities  \n",
       "9   [0.0021867561154067516, -0.004219848196953535,...      0.363270  \n",
       "0   [-0.019770914688706398, 0.011169900186359882, ...      0.314105  \n",
       "1   [-0.007850012741982937, 0.01001765951514244, 0...      0.297908  \n",
       "18  [-0.020315825939178467, 0.0011716989101842046,...      0.295586  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# search through the reviews for a specific product\n",
    "def search_docs(df, user_query, top_n=3, to_print=True):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=\"text-search-curie-query-001\"\n",
    "    )\n",
    "    df[\"similarities\"] = df.curie_search.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    if to_print:\n",
    "        display(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = search_docs(df_bills, \"can i get information on cable company tax revenue\", top_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16f4db34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Taxpayer's Right to View Act of 1993 - Amends the Communications Act of 1934 to prohibit a cable operator from assessing separate charges for any video programming of a sporting, theatrical, or other entertainment event if that event is performed at a facility constructed, renovated, or maintained with tax revenues or by an organization that receives public financial support. Authorizes the Federal Communications Commission and local franchising authorities to make determinations concerning the applicability of such prohibition. Sets forth conditions under which a facility is considered to have been constructed, maintained, or renovated with tax revenues. Considers events performed by nonprofit or public organizations that receive tax subsidies to be subject to this Act if the event is sponsored by, or includes the participation of a team that is part of, a tax exempt organization.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing top result from document search based on user query against the entire knowledge base\n",
    "\n",
    "res[\"summary\"][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f1eb7",
   "metadata": {},
   "source": [
    "Using this approach, you can use embeddings as a search mechanism across documents in a knowledge base. The user can then take the top search result and use it for their downstream task which prompted their initial query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c09e02",
   "metadata": {},
   "source": [
    "## Document \"Zone\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea16ebb",
   "metadata": {},
   "source": [
    "### Use Case Overview\n",
    "This section will go over the document zone use case. This section assumes that document search has already been used to narrow onto one document given the user query. The goal of this section is to show how given a document and a user query, one can find the relevant zones of the document to answer the question or extract the text for future processing such as summarization.\n",
    "\n",
    "### Dataset\n",
    "The dataset used for this section is the CNN/Daily Mail dataset. It is a dataset mainly used for text summarization and question answering tasks. In all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences. More information on the dataset can be found here.4\n",
    "\n",
    "### Schema\n",
    "The schema of the dataset is outlined below.\n",
    "-\tid: a string containing the heximal formatted SHA1 hash of the URL where the story was retrieved from\n",
    "-\tarticle: a string containing the body of the news article\n",
    "-\thighlights: a string containing the highlight of the article as written by the article author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c8a01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(os.getcwd(), 'data', 'cnn_dailymail_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "def96de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            11490\n",
       "article       11490\n",
       "highlights    11490\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bf255",
   "metadata": {},
   "source": [
    "### Document Segmentation\n",
    "\n",
    "It is very common to have documents that can span tens of pages. \n",
    "\n",
    "Due to the token limitation, we cannot pass the entire document into an Azure OpenAI model. For long documents we must chunk the documents into logical segments that can be embedded individually. Therefore, you can segment your large document into smaller chunks based on the document structure, paragraphs, pages etc, and embed each chunk individually. These chunks can be measured against the query embedding, so determine was chunk to “zone in” on for information retrieval or summarization in the next step.\n",
    "\n",
    "For this dataset, the sentences are human readable because they were news articles. Therefore, by splitting the text into every 10 sentences, we can manually create paragraph breaks. This is demonstrated with the splitter function below. \n",
    "\n",
    "We will take the longest article from the dataset and choose to \"zone\" in on it for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cf75b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the dataset by longest text in descending order. Then reindexing the dataset so the new indexes are based of text size in descending order\n",
    "\n",
    "s = dataset.article.str.len().sort_values(ascending=False).index\n",
    "dataset_desc = dataset.reindex(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ec0ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing the longest article in the dataset - will use this document as our \"zoned\" in document to perfrom the search within the document\n",
    "dataset_sample = dataset_desc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf2fccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits text after sentences ending in a period. Combines n sentences per chunk.\n",
    "def splitter(n, s):\n",
    "    pieces = s.split(\". \")\n",
    "    list_out = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba5da53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>og_row</th>\n",
       "      <th>id</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>Hillary Clinton's newborn presidential campaig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>In one case a foundation run by the chairman o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>Canadian businessman Ian Telfer was only shown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>Chelsea Clinton defended her family philanthro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>The foundation listed the donors on its public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>The Clinton Foundation has become lucrative fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>Bill Clinton has made a total of $26 million i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>In 2005 he flew to Kazakhstan with a Canadian ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  og_row                                        id  \\\n",
       "0   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "1   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "2   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "3   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "4   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "5   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "6   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "7   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "\n",
       "                                               chunk  \n",
       "0  Hillary Clinton's newborn presidential campaig...  \n",
       "1  In one case a foundation run by the chairman o...  \n",
       "2  Canadian businessman Ian Telfer was only shown...  \n",
       "3  Chelsea Clinton defended her family philanthro...  \n",
       "4  The foundation listed the donors on its public...  \n",
       "5  The Clinton Foundation has become lucrative fo...  \n",
       "6  Bill Clinton has made a total of $26 million i...  \n",
       "7  In 2005 he flew to Kazakhstan with a Canadian ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmenting document by chunking every 10 sentences\n",
    "\n",
    "df_cols = [\"og_row\", \"id\", \"chunk\"]\n",
    "df_chunked = pd.DataFrame(columns=df_cols)\n",
    "for idx, row in dataset_sample.iterrows():\n",
    "    df_temp = pd.DataFrame(columns=df_cols)\n",
    "    for elem in splitter(10,row[\"article\"]):\n",
    "        df_temp.loc[len(df_temp.index)] = [idx, row[\"id\"], elem]\n",
    "    df_chunked = df_chunked.append(df_temp)\n",
    "\n",
    "df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1df3627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked['chunk'] = df_chunked[\"chunk\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8217b0",
   "metadata": {},
   "source": [
    "Now, we embed each chunk of the news article use a ‘doc’ embedding model (i.e. text-search-curie-doc-001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee12949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked['curie_search'] = df_chunked['chunk'].apply(lambda x : get_embedding(x, engine = TEXT_SEARCH_EMBEDDING_ENGINE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb22ff1",
   "metadata": {},
   "source": [
    "Now, similarly to the previous section. We embed the user query using the associated “query” model (text-serach-query-curie-001). We compare the user query embedding to the embedding for each chunk of the article, to find the chunk that is most like the user query based on cosine similarity and can provide the answer.\n",
    "\n",
    "In this example, we are looking for information specific to the zone in the article. The query is “how much money did bill Clinton make from speaking gigs”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4dd9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>og_row</th>\n",
       "      <th>id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>curie_search</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>Bill Clinton has made a total of $26 million i...</td>\n",
       "      <td>[-0.007038620300590992, 0.010500761680305004, ...</td>\n",
       "      <td>0.449560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4675</td>\n",
       "      <td>0e1d533b46c74279f036efc06f0a8d6e4b0a420f</td>\n",
       "      <td>The Clinton Foundation has become lucrative fo...</td>\n",
       "      <td>[-0.01163650956004858, 0.000741734984330833, -...</td>\n",
       "      <td>0.408853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  og_row                                        id  \\\n",
       "6   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "5   4675  0e1d533b46c74279f036efc06f0a8d6e4b0a420f   \n",
       "\n",
       "                                               chunk  \\\n",
       "6  Bill Clinton has made a total of $26 million i...   \n",
       "5  The Clinton Foundation has become lucrative fo...   \n",
       "\n",
       "                                        curie_search  similarities  \n",
       "6  [-0.007038620300590992, 0.010500761680305004, ...      0.449560  \n",
       "5  [-0.01163650956004858, 0.000741734984330833, -...      0.408853  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# search through the document for a text segment most similar to the query\n",
    "# display top n most similar chunks based on cosine similarity\n",
    "def search_docs(df, user_query, top_n=3, to_print=True):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=\"text-search-curie-query-001\"\n",
    "    )\n",
    "    print(len(embedding))\n",
    "    df[\"similarities\"] = df.curie_search.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    if to_print:\n",
    "        display(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = search_docs(df_chunked, \"how much money did Bill Clinton make from speaking gigs\", top_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a18bd",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "After “zoning in” on a chunk of text using embeddings, the selected text can be extracted as used to create a dynamic prompt that can be passed into an AOAI Completion endpoint for summarization or classification type tasks. Due to the fact the original text was chunked in a manner that will fit within the token limitation, the “zoned in” text is ready to be passed directly to any downstream task. The end-to-end summarization use case is outlined in the End-to-End design above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0385",
   "metadata": {},
   "source": [
    "## Enhancing Prompt Engineering using Embeddings\n",
    "\n",
    "GPT models have acquired lots of general knowledge during training, but often our use cases focus on a more specific topic area that GPT isn't specialized in. In this section we will go over how to use embeddings to improve results with few-shot prompt engineering methods. Using embeddings we can inject domain specific information into a prompt that will enable GPT to more successfully answer the question at hands.\n",
    "\n",
    "Lets walk through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b01ca4",
   "metadata": {},
   "source": [
    "The base GPT-3 model are extremely knowledgable, but may not know the ins and out of the 2015 Austrilian Fashion Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e236353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2015 Australian Fashion Report exposed many clothing brands for ongoing exploitation of overseas workers, including Forever 21, H&M, Zara, and Gap.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Which clothing brands did the 2015 Australian Fashion Report expose for ongoing exploitation of overseas workers?\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    engine=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974fa2e",
   "metadata": {},
   "source": [
    "In fact, the clothing brands that were exposed were Lowes, Industrie, Best & Less and the Just Group. GPT-3 needs assistance here to avoid hallucinations. We rather the model tell us they do not know rather than haullicate. This is essential so we can trust the responses provided by the model.\n",
    "\n",
    "Let's try adding in a statement in our prompt to explicitly state to avoid hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0602e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: Which clothing brands did the 2015 Australian Fashion Report expose for ongoing exploitation of overseas workers?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    engine=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6109d5c",
   "metadata": {},
   "source": [
    "We can help the model answer correctly by providing contextual information into the prompt. When the required context is short, we can fit it into the prompt within the token limitation. \n",
    "\n",
    "Let's update the prompt with the contextual information and explicitly tell the model to refer to the provided text when answering the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2df1c707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lowes, Industrie, Best & Less, and the Just Group were exposed.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context:\n",
    "As Australian Fashion Week comes to a close, a new damning report has named and shamed some of the worst clothing brands sold in Australia and their companies, for the ongoing exploitation of their overseas workers. Lowes, Industrie, Best & Less and the Just Group - which includes Just Jeans, Portmans and Dotti - were identified as some of the worst performing companies by The 2015 Australian Fashion Report. Amongst the best performers were Etiko, Audrey Blue, Cotton On, H&M and Zara. The report assessed the labour rights management systems of 59 companies and 219 brands operating in Australia. The 2015 Australian Fashion Report has named and shamed some of the worst Aussie clothing brands and companies for their ongoing exploitation of overseas workers . Amongst the best performers were Etiko, Audrey Blue, Cotton On, H&M and Zara . It found that only two of the companies could prove they were paying a full living wage to the workers in two of the three production stages of their clothing. None of the 59 companies could prove the workers at their raw material suppliers were paid a living wage. Unlike a country's legally set minimum wage, a living wage ensures that an employee has enough money to cover the necessities - like food, water, electricity and shelter - and still has a little left over for themselves and their dependants. In some countries like Bangladesh, where the minimum wage is as little as US$68 a month and a living wage is US$104, the difference can be made by paying each worker just an additional 30c per t-shirt\n",
    "\n",
    "Q: Which clothing brands did the 2015 Australian Fashion Report expose for ongoing exploitation of overseas workers?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    engine=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3342a",
   "metadata": {},
   "source": [
    "As we can see, this approach of adding extra information into the prompt works well. However, we are limited by ensure the contextual information is small enough to fit within the token limiation.\n",
    "\n",
    "This brings up, how do we know what information to choose to put into the prompt when we have a large body of information when you can't fit it all in?\n",
    "\n",
    "The remainder of this section will go over how to use embeddings to selectively choose the most relevant context out of a large body of text, that will be used to augment a few shot prompt. This method answers the initial question in 2 steps:\n",
    "\n",
    "1. Retrieving the information relevant to the query using the **Embeddings API**.\n",
    "2. Appending teh relevant context to the few shot prompt using the **Completions API**.\n",
    "\n",
    "Let's dive into an example to see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df140e",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "We will use the CNN/Daily Mail dataset once again for this example.\n",
    "\n",
    "The steps that we will execute this approach are as follows:\n",
    "\n",
    "1. Preprocess the knowledge base by splitting into chunk and creating an embedding vector for each chunk\n",
    "2. On receiving a query, embed the query in the same vector space as the context chunks from Step 1. \n",
    "3. Find the most context chunks that are most similar to the query.\n",
    "4. Append the most relevant context chunk to the few shot prompt, and submit the question to GPT-3 with the Completion endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ccddcf",
   "metadata": {},
   "source": [
    "#### Step 1: Preprocess the knowledge base and create context chunks embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1c8b97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>Experts question if  packed out planes are put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liverpool target Neto is also wanted by PSG an...</td>\n",
       "      <td>Fiorentina goalkeeper Neto has been linked wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bruce Jenner will break his silence in a two-h...</td>\n",
       "      <td>Tell-all interview with the reality TV star, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  Ever noticed how plane seats appear to be gett...   \n",
       "1  A drunk teenage boy had to be rescued by secur...   \n",
       "2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "3  Liverpool target Neto is also wanted by PSG an...   \n",
       "4  Bruce Jenner will break his silence in a two-h...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Experts question if  packed out planes are put...  \n",
       "1  Drunk teenage boy climbed into lion enclosure ...  \n",
       "2  Nottingham Forest are close to extending Dougi...  \n",
       "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
       "4  Tell-all interview with the reality TV star, 6...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), 'data', 'cnn_dailymail_data.csv'))\n",
    "df = df[[\"article\", \"highlights\"]]\n",
    "df = df.head(50) #for the sake of the example, we will only take the first 100 articles as our knowledge base to reduce compute time\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f65be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits text after sentences ending in a period. Combines n sentences per chunk.\n",
    "def splitter(n, s):\n",
    "    pieces = s.split(\". \")\n",
    "    list_out = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148eb53",
   "metadata": {},
   "source": [
    "In the next few code blocks, we will take the individual articles are break them into chunks by isolating 10 sentences as a time. Then we will embed those individual chunks. \n",
    "\n",
    "This will result in context chunks and their corresponding vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35e45079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>og_row</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>British Airways has a seat pitch of 31 inches,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Liverpool target Neto is also wanted by PSG an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>47</td>\n",
       "      <td>'It was natural, and I bowled quickly, consist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>48</td>\n",
       "      <td>Tom Lineham scored two interception tries in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>48</td>\n",
       "      <td>Lineham had a good chance to lay the platform ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>49</td>\n",
       "      <td>For years medical experts have warned about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>49</td>\n",
       "      <td>Salt skeptics however believe most Americans a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    og_row                                              chunk\n",
       "0        0  Ever noticed how plane seats appear to be gett...\n",
       "1        0  British Airways has a seat pitch of 31 inches,...\n",
       "2        1  A drunk teenage boy had to be rescued by secur...\n",
       "3        2  Dougie Freedman is on the verge of agreeing a ...\n",
       "4        3  Liverpool target Neto is also wanted by PSG an...\n",
       "..     ...                                                ...\n",
       "150     47  'It was natural, and I bowled quickly, consist...\n",
       "151     48  Tom Lineham scored two interception tries in a...\n",
       "152     48  Lineham had a good chance to lay the platform ...\n",
       "153     49  For years medical experts have warned about th...\n",
       "154     49  Salt skeptics however believe most Americans a...\n",
       "\n",
       "[155 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmenting document by chunking for every 10 sentences\n",
    "# Resulting dataframe will have the og_row or the original index from the knowledge base, so we can refer back to the full article text if needed\n",
    "\n",
    "df_cols = [\"og_row\", \"chunk\"]\n",
    "df_chunked = pd.DataFrame(columns=df_cols)\n",
    "for idx, row in df.iterrows():\n",
    "    df_temp = pd.DataFrame(columns=df_cols)\n",
    "    for elem in splitter(10,row[\"article\"]):\n",
    "        df_temp.loc[len(df_temp.index)] = [idx, elem]\n",
    "    df_chunked = df_chunked.append(df_temp)\n",
    "\n",
    "df_chunked.reset_index(drop=True, inplace=True)\n",
    "df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30f2f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked['curie_search'] = df_chunked['chunk'].apply(lambda x : get_embedding(x, engine = 'text-search-curie-doc-001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68c0485c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>og_row</th>\n",
       "      <th>chunk</th>\n",
       "      <th>curie_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
       "      <td>[-0.0074607389979064465, -0.004383711144328117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>British Airways has a seat pitch of 31 inches,...</td>\n",
       "      <td>[-0.0004175635112915188, 0.0086124949157238, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
       "      <td>[-0.013947161845862865, -0.011407176032662392,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
       "      <td>[-0.006194248795509338, 0.0047354078851640224,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Liverpool target Neto is also wanted by PSG an...</td>\n",
       "      <td>[-0.007845471613109112, 0.0026365553494542837,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>47</td>\n",
       "      <td>'It was natural, and I bowled quickly, consist...</td>\n",
       "      <td>[-0.009540674276649952, 0.0028725622687488794,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>48</td>\n",
       "      <td>Tom Lineham scored two interception tries in a...</td>\n",
       "      <td>[-0.021203014999628067, 0.018296150490641594, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>48</td>\n",
       "      <td>Lineham had a good chance to lay the platform ...</td>\n",
       "      <td>[-0.031285665929317474, 0.009455944411456585, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>49</td>\n",
       "      <td>For years medical experts have warned about th...</td>\n",
       "      <td>[-0.019099948927760124, -0.006712321657687426,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>49</td>\n",
       "      <td>Salt skeptics however believe most Americans a...</td>\n",
       "      <td>[-0.02128716930747032, -0.001804290572181344, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    og_row                                              chunk  \\\n",
       "0        0  Ever noticed how plane seats appear to be gett...   \n",
       "1        0  British Airways has a seat pitch of 31 inches,...   \n",
       "2        1  A drunk teenage boy had to be rescued by secur...   \n",
       "3        2  Dougie Freedman is on the verge of agreeing a ...   \n",
       "4        3  Liverpool target Neto is also wanted by PSG an...   \n",
       "..     ...                                                ...   \n",
       "150     47  'It was natural, and I bowled quickly, consist...   \n",
       "151     48  Tom Lineham scored two interception tries in a...   \n",
       "152     48  Lineham had a good chance to lay the platform ...   \n",
       "153     49  For years medical experts have warned about th...   \n",
       "154     49  Salt skeptics however believe most Americans a...   \n",
       "\n",
       "                                          curie_search  \n",
       "0    [-0.0074607389979064465, -0.004383711144328117...  \n",
       "1    [-0.0004175635112915188, 0.0086124949157238, 0...  \n",
       "2    [-0.013947161845862865, -0.011407176032662392,...  \n",
       "3    [-0.006194248795509338, 0.0047354078851640224,...  \n",
       "4    [-0.007845471613109112, 0.0026365553494542837,...  \n",
       "..                                                 ...  \n",
       "150  [-0.009540674276649952, 0.0028725622687488794,...  \n",
       "151  [-0.021203014999628067, 0.018296150490641594, ...  \n",
       "152  [-0.031285665929317474, 0.009455944411456585, ...  \n",
       "153  [-0.019099948927760124, -0.006712321657687426,...  \n",
       "154  [-0.02128716930747032, -0.001804290572181344, ...  \n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca5020",
   "metadata": {},
   "source": [
    "#### Step 2: On receiving a query, embed the query in the same vector space as the context chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae039077",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"Which clothing brands did the 2015 Australian Fashion Report expose for ongoing exploitation of overseas workers?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804a246",
   "metadata": {},
   "source": [
    "We will take the input query and embed it in the same vector space as the context chunks. We will use the corresponding query model (\"text-search-query-curie-001\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d610ea95",
   "metadata": {},
   "source": [
    "#### Step 3: Find the most context chunks that are most similar to the query.\n",
    "\n",
    "The code sample below combines step 2 and step 3. We will embed the input query and then find the top 3 context chunks that are most similar to the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "deb7199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>og_row</th>\n",
       "      <th>chunk</th>\n",
       "      <th>curie_search</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>15</td>\n",
       "      <td>As Australian Fashion Week comes to a close, a...</td>\n",
       "      <td>[-0.006753021385520697, -0.015173792839050293,...</td>\n",
       "      <td>0.494718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15</td>\n",
       "      <td>Lowes, Industrie, Best &amp; Less and the Just Gro...</td>\n",
       "      <td>[-0.005220436491072178, 0.0013642740668728948,...</td>\n",
       "      <td>0.431739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15</td>\n",
       "      <td>The report comes almost two years after over 1...</td>\n",
       "      <td>[-0.010583952069282532, -0.02157820761203766, ...</td>\n",
       "      <td>0.429292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   og_row                                              chunk  \\\n",
       "46     15  As Australian Fashion Week comes to a close, a...   \n",
       "47     15  Lowes, Industrie, Best & Less and the Just Gro...   \n",
       "48     15  The report comes almost two years after over 1...   \n",
       "\n",
       "                                         curie_search  similarities  \n",
       "46  [-0.006753021385520697, -0.015173792839050293,...      0.494718  \n",
       "47  [-0.005220436491072178, 0.0013642740668728948,...      0.431739  \n",
       "48  [-0.010583952069282532, -0.02157820761203766, ...      0.429292  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# search through the document for a text segment most similar to the query\n",
    "# display top two most similar chunks based on cosine similarity\n",
    "def search_docs(df, user_query, n=3, pprint=True):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=\"text-search-curie-query-001\"\n",
    "    )\n",
    "    print(len(embedding))\n",
    "    df[\"similarities\"] = df.curie_search.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(n)\n",
    "    )\n",
    "    if pprint:\n",
    "        display(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = search_docs(df_chunked, input_query, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914caebd",
   "metadata": {},
   "source": [
    "Let's take a look at the resulting top context chunks that were found through embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6fee98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Australian Fashion Week comes to a close, a new damning report has named and shamed some of the worst clothing brands sold in Australia and their companies, for the ongoing exploitation of their overseas workers Lowes, Industrie, Best & Less and the Just Group - which includes Just Jeans, Portmans and Dotti - were identified as some of the worst performing companies by The 2015 Australian Fashion Report Amongst the best performers were Etiko, Audrey Blue, Cotton On, H&M and Zara The report assessed the labour rights management systems of 59 companies and 219 brands operating in Australia The 2015 Australian Fashion Report has named and shamed some of the worst Aussie clothing brands and companies for their ongoing exploitation of overseas workers  Amongst the best performers were Etiko, Audrey Blue, Cotton On, H&M and Zara  It found that only two of the companies could prove they were paying a full living wage to the workers in two of the three production stages of their clothing None of the 59 companies could prove the workers at their raw material suppliers were paid a living wage Unlike a country's legally set minimum wage, a living wage ensures that an employee has enough money to cover the necessities - like food, water, electricity and shelter - and still has a little left over for themselves and their dependants In some countries like Bangladesh, where the minimum wage is as little as US$68 a month and a living wage is US$104, the difference can be made by paying each worker just an additional 30c per t-shirt\n",
      "\n",
      "\n",
      "Lowes, Industrie, Best & Less and the Just Group - which includes Just Jeans, Portmans and Dotti - were identified as some of the worst performers  'The whole point in our reporting scorecard is if these companies don't have rigours systems in place to mitigate against those risks then you can't be sure that there is no forced labour or child labour in their supply chain,' Gershon Nimbalker, an advocacy manager at Baptist World Aid, said  'A mere 12 per cent of companies could demonstrate any action towards paying wages above the legal minimum, and even then, only for part of their supply chain,' the report states 'Furthermore, 91 per cent of companies still don't know where all their cotton comes from and 75 per cent don't know the source of all their fabrics and inputs 'If companies don't know how and where their products are made, then there's no way for them to ensure that their workers are protected.' Uzbekistan for instance, the world's fifth largest exporter of cotton, was notorious for its  child labour policies which saw children as young as 10 forced to work in the fields until the government recently  improved conditions by renouncing the use of child labour 'on a systematic basis' 'Furthermore, 91 per cent of companies still don't know where all their cotton comes from and 75 per cent don't know the source of all their fabrics and inputs,' the report stated  Gershon Nimbalker, an advocacy manager at Baptist World Aid told Daily Mail Australia that part of the motivation behind the report was to shed light on how many of the world's 165 million children involved in child labour were employed by the fashion industry 'The whole point in our reporting scorecard is if these companies don't have rigours systems in place to mitigate against those risks then you can't be sure that there is no forced labour or child labour in their supply chain,' he said 'There were 61 assessment criteria that we used to grade the companies that were put together with lots of collaboration with international labour rights organisations 'We found all the public information available on the companies - public statements, anything online - and compiled and assessed it before sending a copy to the company and asking for feedback or asking them to tell us what we missed\n",
      "\n",
      "\n",
      "The report comes almost two years after over 1,100 Bangladeshi garment workers died when the Rana Plaza factory collapsed in Bangladesh due to building safety problems  'We found all the public information available on the companies - public statements, anything online - and compiled and assessed it before sending a copy to the company and asking for feedback or asking them to tell us what we missed 'The worst grades basically mean that they have very little public information available about what they're doing to protect workers and on top of that that they haven't engaged with our research process About 75 per cent did engage, but one quarter didn't.' However the report also noted some progress in the industry with companies like Kmart and Cotton On improving their transparency by identifying their suppliers, and H&M, Zara, Country Road and the Sussan Group showing attempts to improve their international worker's pay.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row, idx in res.iterrows():\n",
    "    print(idx[\"chunk\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a7ae8",
   "metadata": {},
   "source": [
    "As we can see, the emebddings API found the most relevant chunks that we can use to enhance our prompt engineering efforts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea1f97",
   "metadata": {},
   "source": [
    "#### Step 4: Append the most relevant context chunk to the few shot prompt, and submit the question to GPT-3 with the Completion endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "068e8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_chunk = res.iloc[0,1] #selecting top content chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "782936d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lowes, Industrie, Best & Less, and the Just Group were identified as some of the worst performing companies by The 2015 Australian Fashion Report.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Designing the prompt to avoid hallucinations, inject the context chunk found using embeddings, and answer the input_query\n",
    "\n",
    "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context:\\n\"\"\" + context_chunk + \"\"\"\n",
    "\n",
    "Q: \"\"\" + input_query + \"\"\" \n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    engine=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d5977",
   "metadata": {},
   "source": [
    "As a result, by combining Embeddings and Completion APIs we can create powerful question answering few shot models that can answer questions on a large knowledge base without needing to finetune. It also understand to answer truthfully and not hallucinate when the answer isn't clear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a382e8",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "Overall, embeddings are an extremely useful model for many different use cases such as text search and text similarity. \n",
    "We find that embeddings are extremely performant for document search and ranking given a query. Additionally, embeddings can aid in pinpointing a specific region in a long document that can answer a user query specific to the document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b9a93",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "Transformer based models tend to have token limitation for the inputs, and Azure OpenAI GPT-3 models are no exception. Currently the token limitation for Azure OpenAI is ~4000 tokens for the text-davinci-002 model. Therefore, long documents require segmentation to be fed into any Azure OpenAI model. Currently, we use the approach of segmenting the document based on common features like sentence boundary detection or if applicable HTML parsing. \n",
    "However, text documents without structure can be challenging. Future work that can be helpful to this area is being able to detect natural paragraph breaks in text through natural features, topic changes, and other features. This will ensure the chunks being based into the model are focused on a singular topic and don’t interrupt the flow of the text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c5934",
   "metadata": {},
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb45fb7",
   "metadata": {},
   "source": [
    "1. Information on BillSUM dataset: https://github.com/FiscalNote/BillSum\n",
    "2. Information on CNN/Daily Mail dataset: https://paperswithcode.com/dataset/cnn-daily-mail-1\n",
    "3. Information on Azure OpenAI Embedding offerings: https://docs.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models#embeddings-models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e713d346594bdc8854b5efeeaa36881066da37f9f361dd11b762eb213cfd5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
