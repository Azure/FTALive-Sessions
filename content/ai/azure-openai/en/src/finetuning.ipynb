{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Fine tuning example\n",
    "In this example we'll try to go over all operations that can be done using the Azure endpoints and their differences with the openAi endpoints (if any).<br>\n",
    "This example focuses on finetuning but touches on the majority of operations that are also available using the API. This example is meant to be a quick way of showing simple operations and is not meant as a finetune model adaptation tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the following section the endpoint and key need to be set up of the next sections to work.<br> Please go to https://portal.azure.com, find your resource and then under \"Resource Management\" -> \"Keys and Endpoints\" look for the \"Endpoint\" value and one of the Keys. They will act as api_base and api_key in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")  # Please add your api key here\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_BASE\") # Please add your endpoint here\n",
    "\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2022-03-01-preview' # this may change in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft Active Directory Authentication\n",
    "Instead of key based authentication, you can use Active Directory to authenticate using credential tokens. Uncomment the next code section to use credential based authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "default_credential = DefaultAzureCredential()\n",
    "token = default_credential.get_token(\"https://cognitiveservices.azure.com\")\n",
    "\n",
    "openai.api_type = 'azure_ad'\n",
    "openai.api_key = token.token\n",
    "openai.api_version = '2022-03-01-preview' # this may change in the future\n",
    "\n",
    "\n",
    "openai.api_base = '' # Please add your endpoint here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "In the next section we will focus on the files operations: importing, listing, retrieving, deleting. For this we need to create 2 temporary files with some sample data. For the sake of simplicity, we will use the same data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the training file: training.jsonl\n",
      "Copying the training file to the validation file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'validation.jsonl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import json\n",
    "\n",
    "training_file_name = 'training.jsonl'\n",
    "validation_file_name = 'validation.jsonl'\n",
    "\n",
    "sample_data = [{\"prompt\": \"When I go to the store, I want an\", \"completion\": \"apple\"},\n",
    "    {\"prompt\": \"When I go to work, I want a\", \"completion\": \"coffe\"},\n",
    "    {\"prompt\": \"When I go home, I want a\", \"completion\": \"soda\"}]\n",
    "\n",
    "print(f'Generating the training file: {training_file_name}')\n",
    "with open(training_file_name, 'w') as training_file:\n",
    "    for entry in sample_data:\n",
    "        json.dump(entry, training_file)\n",
    "        training_file.write('\\n')\n",
    "\n",
    "print(f'Copying the training file to the validation file')\n",
    "shutil.copy(training_file_name, validation_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files: Listing\n",
    "List all of the uploaded files and check for the ones that are named \"training.jsonl\" or \"validation.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing uploaded files.\n",
      "Found 0 total uploaded files in the subscription.\n",
      "Found 0 already uploaded files that match our names.\n"
     ]
    }
   ],
   "source": [
    "print('Checking for existing uploaded files.')\n",
    "results = []\n",
    "files = openai.File.list().data\n",
    "print(f'Found {len(files)} total uploaded files in the subscription.')\n",
    "for item in files:\n",
    "    if item[\"filename\"] in [training_file_name, validation_file_name]:\n",
    "        results.append(item[\"id\"])\n",
    "print(f'Found {len(results)} already uploaded files that match our names.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files: Deleting\n",
    "Let's now delete those found files (if any) since we're going to be re-uploading them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting already uploaded files.\n"
     ]
    }
   ],
   "source": [
    "print(f'Deleting already uploaded files.')\n",
    "for id in results:\n",
    "    openai.File.delete(sid = id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files: Importing & Retrieving\n",
    "Now, let's import our two files ('training.jsonl' and 'validation.jsonl') and keep those IDs since we're going to use them later for finetuning.<br>\n",
    "For this operation we are going to use the cli wrapper which does a bit more checks before uploading and also gives us progress. In addition, after uploading we're going to check the status our import until it has succeeded (or failed if something goes wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 197/197 [00:00<00:00, 74.0kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from training.jsonl: file-9fbaab52f5fd4b228c92f4c5e44f1fb4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 197/197 [00:00<00:00, 172kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from validation.jsonl: file-77e08be3cc7b46c8b69cc96a24b7c97f\n",
      "Status (training_file | validation_file): notRunning | notRunning\n",
      "Status (training_file | validation_file): notRunning | notRunning\n",
      "Status (training_file | validation_file): notRunning | succeeded\n",
      "Status (training_file | validation_file): succeeded | succeeded\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def check_status(training_id, validation_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    valid_status = openai.File.retrieve(validation_id)[\"status\"]\n",
    "    print(f'Status (training_file | validation_file): {train_status} | {valid_status}')\n",
    "    return (train_status, valid_status)\n",
    "\n",
    "#importing our two files\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "validation_id = cli.FineTune._get_or_upload(validation_file_name, True)\n",
    "\n",
    "#checking the status of the imports\n",
    "(train_status, valid_status) = check_status(training_id, validation_id)\n",
    "\n",
    "while train_status not in [\"succeeded\", \"failed\"] or valid_status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status, valid_status) = check_status(training_id, validation_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files: Downloading\n",
    "Now let's download one of the files, the training file for example, to check that everything was in order during importing and all bits are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training file: file-9fbaab52f5fd4b228c92f4c5e44f1fb4\n",
      "b'{\"prompt\": \"When I go to the store, I want an\", \"completion\": \"apple\"}\\n{\"prompt\": \"When I go to work, I want a\", \"completion\": \"coffe\"}\\n{\"prompt\": \"When I go home, I want a\", \"completion\": \"soda\"}\\n'\n"
     ]
    }
   ],
   "source": [
    "print(f'Downloading training file: {training_id}')\n",
    "result = openai.File.download(training_id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune\n",
    "In this section we are going to use the two training and validation files that we imported in the previous section, to train a finetune model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune: Adapt\n",
    "First let's create the finetune adaptation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "The specified base model does not support fine-tuning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m create_args \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraining_file\u001b[39m\u001b[39m\"\u001b[39m: training_id,\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalidation_file\u001b[39m\u001b[39m\"\u001b[39m: validation_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclassification_n_classes\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0m resp \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mFineTune\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcreate_args)\n\u001b[1;32m      9\u001b[0m job_id \u001b[39m=\u001b[39m resp[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m status \u001b[39m=\u001b[39m resp[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.10/site-packages/openai/api_resources/abstract/createable_api_resource.py:39\u001b[0m, in \u001b[0;36mCreateableAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mInvalidAPIType(\u001b[39m\"\u001b[39m\u001b[39mUnsupported API type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m api_type)\n\u001b[0;32m---> 39\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params, request_id\u001b[39m=\u001b[39;49mrequest_id\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[1;32m     44\u001b[0m     response,\n\u001b[1;32m     45\u001b[0m     api_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     plain_old_data\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mplain_old_data,\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.10/site-packages/openai/api_requestor.py:181\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    170\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    172\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    173\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[0;32m--> 181\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.10/site-packages/openai/api_requestor.py:396\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    389\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    390\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    393\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    397\u001b[0m             result\u001b[39m.\u001b[39;49mcontent, result\u001b[39m.\u001b[39;49mstatus_code, result\u001b[39m.\u001b[39;49mheaders, stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    398\u001b[0m         ),\n\u001b[1;32m    399\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.10/site-packages/openai/api_requestor.py:429\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    427\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    430\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The specified base model does not support fine-tuning."
     ]
    }
   ],
   "source": [
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"validation_file\": validation_id,\n",
    "    \"model\": \"curie\",\n",
    "    \"compute_classification_metrics\": True,\n",
    "    \"classification_n_classes\": 3\n",
    "}\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "print(f'Fine-tunning model with jobID: {job_id}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune: Streaming\n",
    "While the job runs, we can subscribe to the streaming events to check the progress of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import datetime\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    status = openai.FineTune.retrieve(job_id).status\n",
    "    print(f\"Stream interrupted. Job is still {status}.\")\n",
    "    return\n",
    "\n",
    "print('Streaming events for the fine-tuning job: {job_id}')\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "events = openai.FineTune.stream_events(job_id)\n",
    "try:\n",
    "    for event in events:\n",
    "        print(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')\n",
    "\n",
    "except Exception:\n",
    "    print(\"Stream interrupted (client disconnected).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune: Listing and Retrieving\n",
    "Now let's check that our operation was successful and in addition we can look at all of the finetuning operations using a list operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(2)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Finetune job {job_id} finished with status: {status}')\n",
    "\n",
    "print('Checking other finetune jobs in the subscription.')\n",
    "result = openai.FineTune.list()\n",
    "print(f'Found {len(result)} finetune jobs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune: Deleting\n",
    "Finally we can delete our finetune job.<br>\n",
    "WARNING: Please skip this step if you want to continue with the next section as the finetune model is needed. (The delete code is commented out by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.FineTune.delete(sid=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployments\n",
    "In this section we are going to create a deployment using the finetune model that we just adapted and then used the deployment to create a simple completion operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Create\n",
    "Let's create a deployment using the fine-tune model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fist let's get the model of the previous job:\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "if result[\"status\"] == 'succeeded':\n",
    "    model = result[\"fine_tuned_model\"]\n",
    "\n",
    "# Now let's create the deployment\n",
    "print(f'Creating a new deployment with model: {model}')\n",
    "result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\n",
    "deployment_id = result[\"id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Retrieving\n",
    "Now let's check the status of the newly created deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Checking for deployment status.')\n",
    "resp = openai.Deployment.retrieve(id=deployment_id)\n",
    "status = resp[\"status\"]\n",
    "print(f'Deployment {deployment_id} is with status: {status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Listing\n",
    "Now because creating a new deployment takes a long time, let's look in the subscription for an already finished deployment that succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('While deployment running, selecting a completed one.')\n",
    "deployment_id = None\n",
    "result = openai.Deployment.list()\n",
    "for deployment in result.data:\n",
    "    if deployment[\"status\"] == \"succeeded\":\n",
    "        deployment_id = deployment[\"id\"]\n",
    "        break\n",
    "\n",
    "if not deployment_id:\n",
    "    print('No deployment with status: succeeded found.')\n",
    "else:\n",
    "    print(f'Found a successful deployment with id: {deployment_id}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completions\n",
    "Now let's send a sample completion to the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sending a test completion job')\n",
    "start_phrase = 'When I go to the store, I want a'\n",
    "response = openai.Completion.create(deployment_id=deployment_id, prompt=start_phrase, max_tokens=4)\n",
    "text = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\n",
    "print(f'\"{start_phrase} {text}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Delete\n",
    "Finally let's delete the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Deleting deployment: {deployment_id}')\n",
    "openai.Deployment.delete(sid=deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e713d346594bdc8854b5efeeaa36881066da37f9f361dd11b762eb213cfd5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
