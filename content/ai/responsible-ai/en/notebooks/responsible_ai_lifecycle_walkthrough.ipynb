{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# End-to-end Responsible AI lifecycle walkthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of this notebook is to walk you through a concrete use case by following the [ML workflow](https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/) and applying the most prominent recommendations from the Responsible AI lifecycle at each stage. This will be done in a cloud-native manner by leveraging [Azure ML MLOps capabilities](https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment).\n",
        "\n",
        "This use case uses the well-known [UCI adult census dataset](https://archive.ics.uci.edu/ml/datasets/Adult). For our purposes, we will use treat this as a loan decision classification problem. We will pretend that the label indicates whether each individual repaid a loan in the past. We will use the data to train a predictor to predict whether previously unseen individuals will repay a loan or not. The assumption is that the model predictions will be used to decide whether an individual should be offered a loan.\n",
        "\n",
        "In what follows, we will go through the stages of the ML workflow sequentially, please consult the acompagnying whitepaper under the whitepaper folder of the repo for more details on the phases of the workflow and the Responsible AI lifecycle activities we take on for each stage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connecting to your Azure ML workspace\n",
        "\n",
        "This step is not needed immidiately but will be very useful for the model training and deployment later on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1631094388154
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "msazureml\taustraliaeast\tmlservices-rg\n"
          ]
        }
      ],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above cell creates a workspace object from the existing workspace. ``Workspace.from_config()`` reads the file ``config.json`` and loads the details into an object named ``ws``. The compute instance has a copy of this file saved in its root directory. If you run the code elsewhere, you'll need to [create the file](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We use the **adult census** dataset that we collect throught the **shap** library. Let's load and have a first look at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1631094420586
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_raw shape: (32561, 12)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education-Num</th>\n",
              "      <th>Marital Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39.0</td>\n",
              "      <td>7</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50.0</td>\n",
              "      <td>6</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38.0</td>\n",
              "      <td>4</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Age  Workclass  Education-Num  Marital Status  Occupation  Relationship  \\\n",
              "0  39.0          7           13.0               4           1             0   \n",
              "1  50.0          6           13.0               2           4             4   \n",
              "2  38.0          4            9.0               0           6             0   \n",
              "3  53.0          4            7.0               2           6             4   \n",
              "4  28.0          4           13.0               2          10             5   \n",
              "\n",
              "   Race  Sex  Capital Gain  Capital Loss  Hours per week  Country  \n",
              "0     4    1        2174.0           0.0            40.0       39  \n",
              "1     4    1           0.0           0.0            13.0       39  \n",
              "2     4    1           0.0           0.0            40.0       39  \n",
              "3     2    1           0.0           0.0            40.0       39  \n",
              "4     2    0           0.0           0.0            40.0        5  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shap # Data is collected through the shap library\n",
        "import pandas as pd\n",
        "\n",
        "# Load the adult cencus dataset\n",
        "X_raw, Y = shap.datasets.adult()\n",
        "df = pd.DataFrame(X_raw, Y)\n",
        "print (\"X_raw shape:\", X_raw.shape)\n",
        "X_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing and cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identifying and handling the missing values\n",
        "\n",
        "In our case, there seems to be no missing values in the dataset so we are happy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1631094421917
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of missing values over all columns\n",
        "X_raw.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "All features above look numeric, however some of them are just \"numeric codes\" and the features they represent are rather categorical. \n",
        "So for more accurate results, we separate categorical features from “real” numeric ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1631094422817
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age               float32\n",
            "Workclass            int8\n",
            "Education-Num     float32\n",
            "Marital Status       int8\n",
            "Occupation           int8\n",
            "Relationship        int64\n",
            "Race                 int8\n",
            "Sex                  int8\n",
            "Capital Gain      float32\n",
            "Capital Loss      float32\n",
            "Hours per week    float32\n",
            "Country              int8\n",
            "dtype: object\n",
            "categorical_features_indices: [ 1  3  4  6  7 11]\n",
            "numeric_features_indices: [ 0  2  8  9 10]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(X_raw.dtypes)\n",
        "categorical_features_indices = np.where(np.logical_or(X_raw.dtypes == np.int8, X_raw.dtypes == np.int32))[0]\n",
        "\n",
        "print('categorical_features_indices:',categorical_features_indices)\n",
        "\n",
        "numeric_features_indices = np.where(X_raw.dtypes == np.float32)[0]\n",
        "numeric_features_indices\n",
        "print('numeric_features_indices:',numeric_features_indices)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "column_transformer = ColumnTransformer ([\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'),\n",
        "    categorical_features_indices),\n",
        "    ('scaler', StandardScaler(),\n",
        "    numeric_features_indices)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1631094461965
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before label encoding: [False False False ... False False  True]\n",
            "After label encoding: [0 0 0 ... 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "print(\"Before label encoding:\",Y) # --> [False False False  ... False False True]\n",
        "Y=le.fit_transform(Y)\n",
        "print(\"After label encoding:\",Y) # --> [0 0 0  ... 0 0 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data split and Features enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1631094635875
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_raw shape: (32561, 12), X_train shape: (26048, 12), X_test shape: (6513, 12)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13077</th>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25002</th>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23777</th>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955</th>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Sex\n",
              "13077    male\n",
              "25002    male\n",
              "23777  female\n",
              "71     female\n",
              "955      male"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "A=X_raw[['Sex']]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(\n",
        "    X_raw, Y, A,\n",
        "    test_size=0.2, random_state=0, stratify=Y)\n",
        "\n",
        "X_train.reset_index(drop=True)\n",
        "X_test.reset_index(drop=True)\n",
        "A_train.reset_index(drop=True)\n",
        "A_test.reset_index(drop=True)\n",
        "\n",
        "print(\"X_raw shape: {}, X_train shape: {}, X_test shape: {}\".format(\n",
        "    X_raw.shape, X_train.shape, X_test.shape))\n",
        "    \n",
        "# test dataframe: features enrichment\n",
        "import pandas as pd\n",
        "\n",
        "pandas_warnings=pd.get_option('mode.chained_assignment')\n",
        "# to avoid warning 'A value is trying to be set on a copy of a slice from a DataFrame'\n",
        "\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "# improve labels by replacing numbers with labels\n",
        "A_test.Sex.loc[(A_test['Sex']==0)] = 'female'\n",
        "A_test.Sex.loc[(A_test['Sex']==1)] = 'male'\n",
        "\n",
        "\n",
        "\n",
        "pd.set_option('mode.chained_assignment', pandas_warnings)\n",
        "\n",
        "A_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fitting a CatBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1630470114664
        }
      },
      "outputs": [],
      "source": [
        "# !pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1631094475625
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "catboost_predictor.score: 0.873637340703209\n"
          ]
        }
      ],
      "source": [
        "# Train your third classification model with Catboost Classifier\n",
        "from catboost import CatBoostClassifier # !pip install catboost==0.18.1\n",
        "\n",
        "model_1 = CatBoostClassifier(\n",
        "    random_seed=42, logging_level=\"Silent\", iterations=150)\n",
        "\n",
        "\n",
        "pipeline_1 = Pipeline(steps=[\n",
        "    ('preprocessor', column_transformer),\n",
        "    ('classifier_CBC', model_1)])\n",
        "\n",
        "catboost_predictor = pipeline_1.fit(X_train, Y_train)\n",
        "\n",
        "print('catboost_predictor.score:', catboost_predictor.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Model transparency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1630478607806
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Using InterpretML\n",
        "\n",
        "from interpret import show\n",
        "from interpret.perf import ROC\n",
        "\n",
        "\n",
        "# 1. Blackbox model performance\n",
        "blackbox_perf = ROC(catboost_predictor .predict_proba).explain_perf(X_test, Y_test, name='Catboost Classifier')\n",
        "\n",
        "# 2. Local Explanations\n",
        "from interpret.blackbox import LimeTabular\n",
        "from interpret import show\n",
        "\n",
        "#Blackbox explainers need a predict function, and optionally a dataset\n",
        "lime = LimeTabular(predict_fn=catboost_predictor.predict_proba, data=X_train, random_state=1)\n",
        "\n",
        "#Pick the instances to explain, optionally pass in labels if you have them\n",
        "lime_local = lime.explain_local(X_test[:5], Y_test[:5], name='LIME')\n",
        "\n",
        "from interpret.blackbox import PartialDependence\n",
        "\n",
        "pdp = PartialDependence(predict_fn=catboost_predictor.predict_proba, data=X_train)\n",
        "pdp_global = pdp.explain_global(name='Partial Dependence')\n",
        "\n",
        "# Show them all in one dashboard\n",
        "show([blackbox_perf, lime_local, pdp_global])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1631096753366
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The option feature_dependence has been renamed to feature_perturbation!\n",
            "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
            "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a046f825dfb3464bbe87d31bffa8e452",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6513 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interpret started at https://abdou-default-compute-5000.francecentral.instances.azureml.ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<raiwidgets.explanation_dashboard.ExplanationDashboard at 0x7ff1aad143c8>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using raiwidgets\n",
        "\n",
        "from raiwidgets import ExplanationDashboard\n",
        "from interpret.ext.blackbox import TabularExplainer\n",
        "\n",
        "# explain predictions on your local machine\n",
        "# \"features\" and \"classes\" fields are optional\n",
        "explainer = TabularExplainer(catboost_predictor, \n",
        "                             X_train)\n",
        "\n",
        "# explain overall model predictions (global explanation)\n",
        "global_explanation = explainer.explain_global(X_test)\n",
        "\n",
        "ExplanationDashboard(global_explanation, catboost_predictor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1631097472100
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Global explenation\n",
        "\n",
        "ranked_global_importance_names = global_explanation.get_ranked_global_names() \n",
        "ranked_global_importance_values = global_explanation.get_ranked_global_values()  \n",
        "shap.summary_plot(np.array([ranked_global_importance_values]), ranked_global_importance_names, plot_type=\"bar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Using Glassbox model: EBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1630473793585
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sparse data not fully supported, will be densified for now, may cause OOM\n"
          ]
        }
      ],
      "source": [
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "\n",
        "seed = 1\n",
        "model_2 = ExplainableBoostingClassifier(random_state=seed, n_jobs=-1)\n",
        "\n",
        "\n",
        "pipeline_2 = Pipeline(steps=[\n",
        "    ('preprocessor', column_transformer),\n",
        "    ('classifier_EBM', model_2)])\n",
        "\n",
        "ebm_predictor = pipeline_2.fit(X_train, Y_train)\n",
        "print('ebm_predictor.score:', ebm_predictor.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Assessing Fairness issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1630470121097
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fairness started at https://abdou-default-compute-5001.francecentral.instances.azureml.ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<raiwidgets.fairness_dashboard.FairnessDashboard at 0x7fd0324082e8>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from raiwidgets import FairnessDashboard\n",
        "Y_pred = catboost_predictor.predict(X_test)\n",
        "FairnessDashboard(sensitive_features=A_test,\n",
        "                  y_true=Y_test,\n",
        "                  y_pred=Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Mitigating fairness issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1631100092268
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from fairlearn.reductions import GridSearch\n",
        "from fairlearn.reductions import DemographicParity, ErrorRate\n",
        "\n",
        "sweep = GridSearch(\n",
        "    model_1,\n",
        "    constraints=DemographicParity(),\n",
        "    grid_size=70)\n",
        "\n",
        "sweep.fit(X_train, Y_train, sensitive_features=A_train.Sex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1631100306635
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fairness started at https://abdou-default-compute-5001.francecentral.instances.azureml.ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<raiwidgets.fairness_dashboard.FairnessDashboard at 0x7ff19599a128>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n"
          ]
        }
      ],
      "source": [
        "from raiwidgets import FairnessDashboard\n",
        "mitigated_predictors = sweep.predictors_\n",
        "\n",
        "ys_mitigated_predictors = {} # it contains (<model_id>, <predictions>) pairs\n",
        "\n",
        "# the original prediction:\n",
        "ys_mitigated_predictors[\"census_unmitigated\"]=catboost_predictor.predict(X_test)\n",
        "\n",
        "base_predictor_name=\"mitigated_predictor_{0}\"\n",
        "model_id=1\n",
        "\n",
        "for mp in mitigated_predictors:\n",
        "    id=base_predictor_name.format(model_id)\n",
        "    ys_mitigated_predictors[id]=mp.predict(X_test)\n",
        "    model_id=model_id+1\n",
        "    \n",
        "FairnessDashboard(\n",
        "    sensitive_features=A_test,\n",
        "    y_true=Y_test,\n",
        "    y_pred=ys_mitigated_predictors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1631094651083
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
        "    false_positive_rate, false_negative_rate,\n",
        "    false_positive_rate_difference, false_negative_rate_difference,\n",
        "    equalized_odds_difference)\n",
        "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
        "\n",
        "# Some helper functions to be used later\n",
        "def get_metrics_df(models_dict, y_true, group):\n",
        "    metrics_dict = {\n",
        "        \"Overall selection rate\": (\n",
        "            lambda x: selection_rate(y_true, x), True),\n",
        "        \"Demographic parity difference\": (\n",
        "            lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n",
        "        \"Demographic parity ratio\": (\n",
        "            lambda x: demographic_parity_ratio(y_true, x, sensitive_features=group), True),\n",
        "        \"------\": (lambda x: \"\", True),\n",
        "        \"Overall balanced error rate\": (\n",
        "            lambda x: 1-balanced_accuracy_score(y_true, x), True),\n",
        "        \"Balanced error rate difference\": (\n",
        "            lambda x: MetricFrame(balanced_accuracy_score, y_true, x, sensitive_features=group).difference(method='between_groups'), True),\n",
        "        \" ------\": (lambda x: \"\", True),\n",
        "        \"False positive rate difference\": (\n",
        "            lambda x: false_positive_rate_difference(y_true, x, sensitive_features=group), True),\n",
        "        \"False negative rate difference\": (\n",
        "            lambda x: false_negative_rate_difference(y_true, x, sensitive_features=group), True),\n",
        "        \"Equalized odds difference\": (\n",
        "            lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n",
        "        \"  ------\": (lambda x: \"\", True),\n",
        "        \"Overall AUC\": (\n",
        "            lambda x: roc_auc_score(y_true, x), False),\n",
        "        \"AUC difference\": (\n",
        "            lambda x: MetricFrame(roc_auc_score, y_true, x, sensitive_features=group).difference(method='between_groups'), False),\n",
        "    }\n",
        "    df_dict = {}\n",
        "    for metric_name, (metric_func, use_preds) in metrics_dict.items():\n",
        "        df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n",
        "                                for model_name, (preds, scores) in models_dict.items()]\n",
        "    return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1631094660358
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
            "You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>catboost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Overall selection rate</th>\n",
              "      <td>0.340396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Demographic parity difference</th>\n",
              "      <td>0.305226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Demographic parity ratio</th>\n",
              "      <td>0.306794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>------</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Overall balanced error rate</th>\n",
              "      <td>0.15928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Balanced error rate difference</th>\n",
              "      <td>0.0426844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>------</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False positive rate difference</th>\n",
              "      <td>0.195099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False negative rate difference</th>\n",
              "      <td>0.10973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equalized odds difference</th>\n",
              "      <td>0.195099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>------</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Overall AUC</th>\n",
              "      <td>0.927269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AUC difference</th>\n",
              "      <td>0.0349377</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 catboost\n",
              "Overall selection rate           0.340396\n",
              "Demographic parity difference    0.305226\n",
              "Demographic parity ratio         0.306794\n",
              "------                                   \n",
              "Overall balanced error rate       0.15928\n",
              "Balanced error rate difference  0.0426844\n",
              " ------                                  \n",
              "False positive rate difference   0.195099\n",
              "False negative rate difference    0.10973\n",
              "Equalized odds difference        0.195099\n",
              "  ------                                 \n",
              "Overall AUC                      0.927269\n",
              "AUC difference                  0.0349377"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Scores on test set\n",
        "test_scores = catboost_predictor.predict_proba(X_test)[:, 1]\n",
        "# Predictions (0 or 1) on test set\n",
        "test_preds = (test_scores >= np.mean(Y_train)) * 1\n",
        "# Metrics\n",
        "models_dict = {\"catboost\": (test_preds, test_scores)}\n",
        "\n",
        "get_metrics_df(models_dict, Y_test, A_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Model deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Model registration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "First, we save the model to a file and verify we can properly load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1631001783206
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open('catboost_predictor', 'wb') as file:\n",
        "    pickle.dump(catboost_predictor,file)\n",
        "\n",
        "# Checking the model can be properly loaded from the file\n",
        "with open('catboost_predictor', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "loaded_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Next, we use the Model class to save the model to Azure by providing the path to the model_file we just saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1631001951986
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering model catboost_predictor\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "# Register model\n",
        "model = Model.register(ws, model_name=\"catboost_predictor\", model_path=\"./catboost_predictor\")\n",
        "print('Name:', model.name)\n",
        "print('Version:', model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 2: Define an entry script\n",
        "\n",
        "Please follow the instructions on the whitepaper to create a first entry script ``score.py``  under ``source_dir`` directory before continuing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 3: Define an inference configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "gather": {
          "logged": 1631026874015
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "env = Environment(name=\"project_environment\")\n",
        "\n",
        "conda_dep = CondaDependencies()\n",
        "\n",
        "# Installs azure-ml-api-sdk package\n",
        "conda_dep.add_pip_package(\"azure-ml-api-sdk\")\n",
        "\n",
        "# Installs catboost package\n",
        "conda_dep.add_pip_package(\"catboost\")\n",
        "\n",
        "# Installs numpy package\n",
        "conda_dep.add_pip_package(\"numpy\")\n",
        "\n",
        "# Installs pandas package\n",
        "conda_dep.add_pip_package(\"pandas\")\n",
        "\n",
        "# Installs sklearn package\n",
        "conda_dep.add_pip_package(\"sklearn\")\n",
        "\n",
        "# Adds dependencies to PythonSection of env\n",
        "env.python.conda_dependencies=conda_dep\n",
        "\n",
        "inference_config = InferenceConfig(\n",
        "    environment=env,\n",
        "    source_directory=\"./source_dir\",\n",
        "    entry_script=\"./score.py\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 4: Define a deployment configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "gather": {
          "logged": 1631024173967
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.webservice import LocalWebservice\n",
        "\n",
        "deployment_config = LocalWebservice.deploy_configuration(port=6789)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 5: Deploy your ML model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "gather": {
          "logged": 1631024208457
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model catboost_predictor:1 to /tmp/azureml_uikco5i9/catboost_predictor/1\n",
            "Generating Docker build context.\n",
            "Package creation Succeeded\n",
            "Logging into Docker registry abdoudefaultcontainerregistry.azurecr.io\n",
            "Logging into Docker registry abdoudefaultcontainerregistry.azurecr.io\n",
            "Building Docker image from Dockerfile...\n",
            "Step 1/5 : FROM abdoudefaultcontainerregistry.azurecr.io/azureml/azureml_c2add3fa2bf2efed1a677650ec3e09e9\n",
            " ---> e53cd91d6f55\n",
            "Step 2/5 : COPY azureml-app /var/azureml-app\n",
            " ---> 771555cb3126\n",
            "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjU2N2Q3MGNkLWVlMTYtNDlkOS1iZTlmLTMxMDg4YjY1ZTUzYiIsInJlc291cmNlR3JvdXBOYW1lIjoiYWJkb3UtcmVzb3VyY2VzLWdyb3VwIiwiYWNjb3VudE5hbWUiOiJhYmRvdS1henVyZS1tbC13b3Jrc3BhY2UiLCJ3b3Jrc3BhY2VJZCI6Ijg4OWVkZjRlLTBhMTktNDY3Yi05NTZiLWJiZmY4YTI0OWY4YyJ9LCJtb2RlbHMiOnt9LCJtb2RlbHNJbmZvIjp7fX0= | base64 --decode > /var/azureml-app/model_config_map.json\n",
            " ---> Running in 237b9f91108b\n",
            " ---> 734c3fc6ef74\n",
            "Step 4/5 : RUN mv '/var/azureml-app/tmp828hhsga.py' /var/azureml-app/main.py\n",
            " ---> Running in 74b8f37a36b9\n",
            " ---> eeb16e8bca56\n",
            "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
            " ---> Running in cc075f163760\n",
            " ---> 6c017d189de2\n",
            "Successfully built 6c017d189de2\n",
            "Successfully tagged myservice:latest\n",
            "Container has been successfully cleaned up.\n",
            "Image sha256:b0f2df7e91874c7a782395cba947c0cb392560df58349a2500d39347be16cd17 successfully removed.\n",
            "Starting Docker container...\n",
            "Docker container running.\n",
            "Checking container health...\n",
            "Local webservice is running at http://localhost:6789\n",
            "2021-09-07T14:16:40,678311189+00:00 - gunicorn/run \n",
            "File not found: /var/azureml-app/.\n",
            "Starting HTTP server\n",
            "2021-09-07T14:16:40,682149953+00:00 - iot-server/run \n",
            "2021-09-07T14:16:40,682249752+00:00 - nginx/run \n",
            "2021-09-07T14:16:40,683114344+00:00 - rsyslog/run \n",
            "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
            "2021-09-07T14:16:40,769956114+00:00 - iot-server/finish 1 0\n",
            "2021-09-07T14:16:40,771245002+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
            "Starting gunicorn 20.1.0\n",
            "Listening at: http://127.0.0.1:31311 (14)\n",
            "Using worker: sync\n",
            "worker timeout is set to 300\n",
            "Booting worker with pid: 42\n",
            "SPARK_HOME not set. Skipping PySpark Initialization.\n",
            "Initializing logger\n",
            "2021-09-07 14:16:42,395 | root | INFO | Starting up app insights client\n",
            "logging socket was found. logging is available.\n",
            "logging socket was found. logging is available.\n",
            "2021-09-07 14:16:42,395 | root | INFO | Starting up request id generator\n",
            "2021-09-07 14:16:42,395 | root | INFO | Starting up app insight hooks\n",
            "2021-09-07 14:16:42,395 | root | INFO | Invoking user's init function\n",
            "Model Path is\n",
            "no request id,Model Path is\n",
            "\n",
            "azureml-models/catboost_predictor/1/catboost_predictor\n",
            "no request id,azureml-models/catboost_predictor/1/catboost_predictor\n",
            "\n",
            "no request id,/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "\n",
            "no request id,/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "\n",
            "no request id,/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "\n",
            "no request id,/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "\n",
            "2021-09-07 14:16:42,781 | root | INFO | Users's init has completed successfully\n",
            "/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/azureml-envs/azureml_2dec41fef49afc81948f597dc49752ef/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "2021-09-07 14:16:42,783 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
            "2021-09-07 14:16:42,783 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
            "2021-09-07 14:16:42,783 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "service = Model.deploy(\n",
        "    ws,\n",
        "    \"myservice\",\n",
        "    [model],\n",
        "    inference_config,\n",
        "    deployment_config,\n",
        "    overwrite=True,\n",
        ")\n",
        "service.wait_for_deployment(show_output=True)\n",
        "\n",
        "print(service.get_logs())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 6: Call into our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "gather": {
          "logged": 1631024257008
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': [0, 0, 0, 0, 0], 'message': 'Successfully classified loan'}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "uri = service.scoring_uri\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "json_test = X_test.head().to_json()\n",
        "data = json.dumps(json_test)\n",
        "response = requests.post(uri, data=data, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "gather": {
          "logged": 1631024422646
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_test[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "It looks like our model works and we correctly classified the first five entries of the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Deployment to Azure Container Instance: repeating steps  4 - 6 with ACI compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "gather": {
          "logged": 1631027060810
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
            "Running\n",
            "2021-09-07 15:01:33+00:00 Creating Container Registry if not exists.\n",
            "2021-09-07 15:01:33+00:00 Registering the environment.\n",
            "2021-09-07 15:01:36+00:00 Use the existing image.\n",
            "2021-09-07 15:01:36+00:00 Generating deployment configuration.\n",
            "2021-09-07 15:01:37+00:00 Submitting deployment to compute.\n",
            "2021-09-07 15:01:42+00:00 Checking the status of deployment aciservice..\n",
            "2021-09-07 15:04:12+00:00 Checking the status of inference endpoint aciservice.\n",
            "Succeeded\n",
            "ACI service creation operation finished, operation \"Succeeded\"\n",
            "Healthy\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# Step 4: Define deployment config\n",
        "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
        "\n",
        "# Step 5: Deploying the model\n",
        "service = Model.deploy(ws, \"aciservice\", [model], inference_config, deployment_config)\n",
        "service.wait_for_deployment(show_output = True)\n",
        "\n",
        "# Step 6: Consuming the endpoint\n",
        "uri = service.scoring_uri\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "json_test = X_test.head().to_json()\n",
        "data = json.dumps(json_test)\n",
        "response = requests.post(uri, data=data, headers=headers)\n",
        "print(response.json())\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
