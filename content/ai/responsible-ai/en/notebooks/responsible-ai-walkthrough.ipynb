{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI Model Training\n",
    "責任ある AI を考慮したモデル構築を行います。データは [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) をローンの負債・不履行の履歴データとして擬似的に扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アジェンダ\n",
    "1. データ準備\n",
    "    - 利用するサンプルデータのロード、データ前処理のパイプライン作成を行います。\n",
    "2. モデル構築\n",
    "    - 説明性、解釈可能性、公平性を考慮したモデルを構築します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データ準備\n",
    "shap ライブラリから提供されているデータ前処理済みのデータを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from helper import ebm_preserve_global, ebm_preserve_local, ebm_preserve_perf\n",
    "\n",
    "# Load the adult cencus dataset\n",
    "X_raw, Y = shap.datasets.adult()\n",
    "print (\"X_raw shape:\", X_raw.shape)\n",
    "X_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値の有無\n",
    "X_raw.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(X_raw.dtypes)\n",
    "categorical_features_indices = np.where(np.logical_or(X_raw.dtypes == np.int8, X_raw.dtypes == np.int32, X_raw.dtypes == np.int64))[0]\n",
    "\n",
    "print('カテゴリ変数のインデックス:',categorical_features_indices)\n",
    "\n",
    "numeric_features_indices = np.where(X_raw.dtypes == np.float32)[0]\n",
    "numeric_features_indices\n",
    "print('数値変数のインデックス:',numeric_features_indices)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# データ加工パイプライン\n",
    "column_transformer = ColumnTransformer ([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'),\n",
    "    categorical_features_indices),\n",
    "    ('scaler', StandardScaler(),\n",
    "    numeric_features_indices)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_raw.columns.to_list()\n",
    "feature_types = ['categorical' if i in categorical_features_indices else 'continuous' for i in range(len(feature_names))]\n",
    "print(list(zip(feature_names, feature_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le=LabelEncoder()\n",
    "print(\"ラベルエンコーディング前:\",Y) # --> [False False False  ... False False True]\n",
    "Y=le.fit_transform(Y)\n",
    "print(\"ラベルエンコーディング後:\",Y) # --> [0 0 0  ... 0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "A=X_raw[['Sex']] # Sensitive な変数\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(\n",
    "    X_raw, Y, A,\n",
    "    test_size=0.2, random_state=0, stratify=Y)\n",
    "\n",
    "X_train.reset_index(drop=True)\n",
    "X_test.reset_index(drop=True)\n",
    "A_train.reset_index(drop=True)\n",
    "A_test.reset_index(drop=True)\n",
    "\n",
    "print(\"X_raw shape: {}, X_train shape: {}, X_test shape: {}\".format(\n",
    "    X_raw.shape, X_train.shape, X_test.shape))\n",
    "    \n",
    "# test dataframe: features enrichment\n",
    "import pandas as pd\n",
    "\n",
    "pandas_warnings=pd.get_option('mode.chained_assignment')\n",
    "# to avoid warning 'A value is trying to be set on a copy of a slice from a DataFrame'\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# improve labels by replacing numbers with labels\n",
    "A_test.Sex.loc[(A_test['Sex']==0)] = 'female'\n",
    "A_test.Sex.loc[(A_test['Sex']==1)] = 'male'\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('mode.chained_assignment', pandas_warnings)\n",
    "\n",
    "A_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 通常の勾配ブースティングモデルの構築\n",
    "CatBoost によるモデル学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your third classification model with Catboost Classifier\n",
    "from catboost import CatBoostClassifier # !pip install catboost==0.18.1\n",
    "\n",
    "model_1 = CatBoostClassifier(\n",
    "    random_seed=42, logging_level=\"Silent\", iterations=150)\n",
    "\n",
    "\n",
    "pipeline_1 = Pipeline(steps=[\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier_CBC', model_1)])\n",
    "\n",
    "catboost_predictor = pipeline_1.fit(X_train, Y_train)\n",
    "\n",
    "print('catboost_predictor.score:', catboost_predictor.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 解釈可能性の高いモデルの構築 (Glass-box approach)\n",
    "[interpret]() ライブラリに含まれる一般化加法モデルの推定アルゴリズム Explainable Boosting Machine (aka EBM) を用いたモデル開発を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret.perf import ROC\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "seed = 1234\n",
    "\n",
    "#  No pipeline needed due to EBM handling string datatypes\n",
    "ebm_predictor = ExplainableBoostingClassifier(feature_types = feature_types, random_state=seed, interactions=4)\n",
    "ebm_predictor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_global = ebm_predictor.explain_global(name='EBM')\n",
    "\n",
    "ebm_preserve_global(ebm_global, 'ebm_global') # for vscode\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_local = ebm_predictor.explain_local(X_test[:10], Y_test[:10], name='EBM')\n",
    "\n",
    "ebm_preserve_local(ebm_local, 'ebm_local') # for vscode\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_perf = ROC(ebm_predictor.predict_proba).explain_perf(X_test, Y_test, name='EBM')\n",
    "ebm_preserve_perf(ebm_perf, 'ebm_perf') # for vscode\n",
    "show(ebm_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 構築済みモデルに対する説明性の付与 (Black-box approach)\n",
    "構築済みモデルをブラックボックスとして扱いつつ、説明性を付与していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 説明性の付与と誤差分析\n",
    "[Interpret-community](https://github.com/interpretml/interpret-community) ライブラリを用いて、Catboost で構築された勾配ブースティングモデルへの説明性の付与と誤差分析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ExplanationDashboard\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "# explain predictions on your local machine\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "explainer = TabularExplainer(catboost_predictor, \n",
    "                             X_train)\n",
    "\n",
    "# explain overall model predictions (global explanation)\n",
    "global_explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# ExplanationDashboard(global_explanation, catboost_predictor)\n",
    "ExplanationDashboard(global_explanation, catboost_predictor, dataset=X_test, true_y=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にモデルの誤差を分析して、潜在的なリスクが何か確認を行います。精度が悪いコホートについては、データ量や質の改善を行い、潜在的なリスクを軽減していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ErrorAnalysisDashboard\n",
    "ErrorAnalysisDashboard(global_explanation, catboost_predictor, dataset=X_test, true_y=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、これらの一連の流れは統合されたダッシュボードで表現することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ResponsibleAIDashboard\n",
    "from responsibleai import RAIInsights\n",
    "\n",
    "train_data = X_train.copy()\n",
    "train_data[\"income\"] = Y_train\n",
    "\n",
    "test_data =  X_test.copy()\n",
    "test_data[\"income\"] = Y_test\n",
    "\n",
    "categorical_features = X_train.columns[categorical_features_indices].to_list()\n",
    "target_feature = \"income\"\n",
    "\n",
    "\n",
    "# データや目的変数などの情報\n",
    "rai_insights = RAIInsights(pipeline_1, train_data, test_data, target_feature, 'classification',\n",
    "                               categorical_features=categorical_features, maximum_rows_for_test=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル説明性 (InterpretML)\n",
    "rai_insights.explainer.add()\n",
    "# モデル誤差解析 (Error Analysis)\n",
    "rai_insights.error_analysis.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_insights.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponsibleAIDashboard(rai_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 公平性の評価と軽減\n",
    "Fairlearn を用いてモデルの公平性の評価を行い、必要に応じて不公平性を軽減するモデルを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 公平性の評価\n",
    "from raiwidgets import FairnessDashboard\n",
    "Y_pred = catboost_predictor.predict(X_test)\n",
    "FairnessDashboard(sensitive_features=A_test,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不公平性を軽減する方法として `削除` と `後処理` の 2 種類があります。ここでは `削除` を用いて想いづけされたデータを用いたモデルの再学習を行います。その後、精度と不均衡のトレードオフを確認し、適したモデルを選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不公平性の軽減\n",
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity, ErrorRate\n",
    "\n",
    "sweep = GridSearch(\n",
    "    model_1,\n",
    "    constraints=DemographicParity(),\n",
    "    grid_size=70)\n",
    "\n",
    "sweep.fit(X_train, Y_train, sensitive_features=A_train.Sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import FairnessDashboard\n",
    "mitigated_predictors = sweep.predictors_\n",
    "\n",
    "ys_mitigated_predictors = {} # it contains (<model_id>, <predictions>) pairs\n",
    "\n",
    "# the original prediction:\n",
    "ys_mitigated_predictors[\"census_unmitigated\"]=catboost_predictor.predict(X_test)\n",
    "\n",
    "base_predictor_name=\"mitigated_predictor_{0}\"\n",
    "model_id=1\n",
    "\n",
    "for mp in mitigated_predictors:\n",
    "    id=base_predictor_name.format(model_id)\n",
    "    ys_mitigated_predictors[id]=mp.predict(X_test)\n",
    "    model_id=model_id+1\n",
    "    \n",
    "FairnessDashboard(\n",
    "    sensitive_features=A_test,\n",
    "    y_true=Y_test,\n",
    "    y_pred=ys_mitigated_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
